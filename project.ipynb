{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnafJmRzRJ6b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zApz3MNe9TX",
        "outputId": "c030fbf7-e2d8-48f1-a013-d9c58984072f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcLy-3uegKwI",
        "outputId": "74a7585b-ae9c-466e-ff08-e23279aec1a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/project_folder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5InVrf1ZgQeh",
        "outputId": "cb5f8150-67b9-455a-e36c-436b96c414f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1hJv0qBvgMO6MUfTgLSXQA0guuUrUM98B/project_folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/peterwang512/CNNDetection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhiwVnu6gUjt",
        "outputId": "2a6d671a-6485-46fe-f59d-d883da26311b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CNNDetection' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HZr7Y29ghJk",
        "outputId": "4ab79dbf-bb7a-4bee-9b6b-9bb8d4d8f562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python <executable_file.py>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTSTI0KFhyrt",
        "outputId": "7e71caae-58c3-40b9-bd72-35962d8af640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 0: ` python <executable_file.py>'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip gdrive/My Drive/project_folder/CNN_synth_testset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUt89gu2j7JD",
        "outputId": "7322ea7e-e08c-4b9c-ab2e-d06e409c0142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open gdrive/My, gdrive/My.zip or gdrive/My.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip gdrive/My Drive/project_folder/CNN_synth_testset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff1OZBdDkZvl",
        "outputId": "89e0502c-ca22-48f3-cd7e-327d07408227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open gdrive/My, gdrive/My.zip or gdrive/My.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==1.11.0 torchvision"
      ],
      "metadata": {
        "id": "2lrcDapyhroW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F50XY0ilj0GS",
        "outputId": "234c9c21-abd7-4adb-9f69-6a64cc016e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "os.chdir('/content/gdrive/MyDrive/project_folder/CNNDetection')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fqZsHYOqj6Bp",
        "outputId": "d83eace8-2211-47ba-d662-d99c70c00836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/.shortcut-targets-by-id/1hJv0qBvgMO6MUfTgLSXQA0guuUrUM98B/project_folder/CNNDetection'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgucMtE2nLmR",
        "outputId": "848a3aa9-4eb7-4e40-f9bf-1a036732a775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: opencv_python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->-r requirements.txt (line 6)) (4.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->-r requirements.txt (line 7)) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash weights/download_weights.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFsH4do2scXb",
        "outputId": "8d159b46-f932-48dd-f7d2-532f9da02f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-05 20:51:18--  https://www.dropbox.com/s/2g2jagq2jn1fd0i/blur_jpg_prob0.5.pth?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.71.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.71.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/2g2jagq2jn1fd0i/blur_jpg_prob0.5.pth [following]\n",
            "--2022-06-05 20:51:19--  https://www.dropbox.com/s/raw/2g2jagq2jn1fd0i/blur_jpg_prob0.5.pth\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com/cd/0/inline/Bmo4bfgrwd7FEMBSCr2jGvCLV3gkzDElPDFzNUMaTd8O9QvYRBObj_o2u16WmrLp9mScYfqjOvbCOsoFvLuBYp7V9WpEGDZ_uoYYmo2De1Zd-WpNwCKkMpY69BQ_aPSoeq_HnpWOgqugP9o9FqoYxQsvmJohpfU5a_OuaXybD5mIOA/file# [following]\n",
            "--2022-06-05 20:51:19--  https://uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com/cd/0/inline/Bmo4bfgrwd7FEMBSCr2jGvCLV3gkzDElPDFzNUMaTd8O9QvYRBObj_o2u16WmrLp9mScYfqjOvbCOsoFvLuBYp7V9WpEGDZ_uoYYmo2De1Zd-WpNwCKkMpY69BQ_aPSoeq_HnpWOgqugP9o9FqoYxQsvmJohpfU5a_OuaXybD5mIOA/file\n",
            "Resolving uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com (uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com (uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bmq3yRF8c1vgTwJPJ5k5_6NSGyP2sI3U7KyV-wtbSwq6C98kYeLpI5qGBNjAwVtrqReaAnA-n0h1SOWATXDX2x3ylerKUr8e_Ilg6V_SEqB_Ym33caw4Hasrlbyt0_Cnc7ZQe-hxjKZ3MGqkYRmZ8zKCsSnyjUPFNligbyuTeov-zRm3rESGRHG9nXbE541G8CVilpr8usOlwm-Mj8xWpp-j9K0yN36ICPjuCAU6T6bBDS6WNY4O-vOHuQ7355ZMmh9Gk1aqy-pwh_6SmXV92ydqmgQHFcgIWusocNRImtvCe_gVLaDhqxw5GRnyXv4v_AxuBJokWBoSrxxvhaL3z1XBAvjpHwlA5aa914LCoYPGbAUfVy_jmWcZTA5HxVKLKweZvT3MC_vKm4uPCwjcvzq_V28pKIC3qahymebNFg4irg/file [following]\n",
            "--2022-06-05 20:51:20--  https://uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com/cd/0/inline2/Bmq3yRF8c1vgTwJPJ5k5_6NSGyP2sI3U7KyV-wtbSwq6C98kYeLpI5qGBNjAwVtrqReaAnA-n0h1SOWATXDX2x3ylerKUr8e_Ilg6V_SEqB_Ym33caw4Hasrlbyt0_Cnc7ZQe-hxjKZ3MGqkYRmZ8zKCsSnyjUPFNligbyuTeov-zRm3rESGRHG9nXbE541G8CVilpr8usOlwm-Mj8xWpp-j9K0yN36ICPjuCAU6T6bBDS6WNY4O-vOHuQ7355ZMmh9Gk1aqy-pwh_6SmXV92ydqmgQHFcgIWusocNRImtvCe_gVLaDhqxw5GRnyXv4v_AxuBJokWBoSrxxvhaL3z1XBAvjpHwlA5aa914LCoYPGbAUfVy_jmWcZTA5HxVKLKweZvT3MC_vKm4uPCwjcvzq_V28pKIC3qahymebNFg4irg/file\n",
            "Reusing existing connection to uc23b0e9bf2bbf68d50d605ea3fe.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 282442597 (269M) [application/octet-stream]\n",
            "Saving to: ‘./weights/blur_jpg_prob0.5.pth’\n",
            "\n",
            "./weights/blur_jpg_ 100%[===================>] 269.36M  29.6MB/s    in 9.3s    \n",
            "\n",
            "2022-06-05 20:51:30 (29.1 MB/s) - ‘./weights/blur_jpg_prob0.5.pth’ saved [282442597/282442597]\n",
            "\n",
            "--2022-06-05 20:51:33--  https://www.dropbox.com/s/h7tkpcgiwuftb6g/blur_jpg_prob0.1.pth?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.71.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.71.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/h7tkpcgiwuftb6g/blur_jpg_prob0.1.pth [following]\n",
            "--2022-06-05 20:51:34--  https://www.dropbox.com/s/raw/h7tkpcgiwuftb6g/blur_jpg_prob0.1.pth\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com/cd/0/inline/BmqFyq8iKomqK_o5clONWqkePQZP1767Gdj5Qu03kaNc59mifxhRK6YGOwlFeMQfE73nb32Juae0tHOfHlMCjGytNhhY7lonB-mGdyb6QWSWtyAf5763mdHugDrx9vEk1eMJhpW1E8U96YNiIy4MNYvoxBkGsRz-gRJzTdu2aW0JCA/file# [following]\n",
            "--2022-06-05 20:51:34--  https://uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com/cd/0/inline/BmqFyq8iKomqK_o5clONWqkePQZP1767Gdj5Qu03kaNc59mifxhRK6YGOwlFeMQfE73nb32Juae0tHOfHlMCjGytNhhY7lonB-mGdyb6QWSWtyAf5763mdHugDrx9vEk1eMJhpW1E8U96YNiIy4MNYvoxBkGsRz-gRJzTdu2aW0JCA/file\n",
            "Resolving uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com (uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com)... 162.125.71.15, 2620:100:6022:15::a27d:420f\n",
            "Connecting to uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com (uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com)|162.125.71.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bmo_t1e74UwL37Cn2XkTTu_srboHG30vL-9NWxcKhuRGoCbyy9WI-NPd697JUGATloMOPi6nxbZ-8SggyuAiOKmTzz8Y4e7nmyJ5_Fah1oJ6k20bIjevJcaZQAQ2sM3jhdtV8RIasNik28xKIWP9Hw1UROorzugdnJeSrDZfEUpMEnqWzKea6vJAQnzdNTkPRW3qJ1Trh45Bon7KAYEfzqsKjK1WBsUNUvbmH61Ne3Z3yhimpFrsTqibsNrhqfrXNdBPjgPbTQNIntgXUZrgkZRKqcxAHTn9WlZpFJY6kfNsov92aB3k-vwUVntFS-vQXSvd9RxWD2msymjE6wn2S-wbPLA-IQ_mMM9L0Ug6-xTA3o5VwGPwIVrGt61jW86iUYFP3USSST2XeXIpdPKqfnvdCbZD7RbBS4ZXeJSztPljIA/file [following]\n",
            "--2022-06-05 20:51:35--  https://uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com/cd/0/inline2/Bmo_t1e74UwL37Cn2XkTTu_srboHG30vL-9NWxcKhuRGoCbyy9WI-NPd697JUGATloMOPi6nxbZ-8SggyuAiOKmTzz8Y4e7nmyJ5_Fah1oJ6k20bIjevJcaZQAQ2sM3jhdtV8RIasNik28xKIWP9Hw1UROorzugdnJeSrDZfEUpMEnqWzKea6vJAQnzdNTkPRW3qJ1Trh45Bon7KAYEfzqsKjK1WBsUNUvbmH61Ne3Z3yhimpFrsTqibsNrhqfrXNdBPjgPbTQNIntgXUZrgkZRKqcxAHTn9WlZpFJY6kfNsov92aB3k-vwUVntFS-vQXSvd9RxWD2msymjE6wn2S-wbPLA-IQ_mMM9L0Ug6-xTA3o5VwGPwIVrGt61jW86iUYFP3USSST2XeXIpdPKqfnvdCbZD7RbBS4ZXeJSztPljIA/file\n",
            "Reusing existing connection to uc038e24b93643c7306bc82e886b.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 282442597 (269M) [application/octet-stream]\n",
            "Saving to: ‘./weights/blur_jpg_prob0.1.pth’\n",
            "\n",
            "./weights/blur_jpg_ 100%[===================>] 269.36M  16.4MB/s    in 16s     \n",
            "\n",
            "2022-06-05 20:51:52 (17.2 MB/s) - ‘./weights/blur_jpg_prob0.1.pth’ saved [282442597/282442597]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "parser.add_argument('-f','--file', default='examples_realfakedir')\n",
        "parser.add_argument('-m','--model_path', type=str, default='weights/blur_jpg_prob0.5.pth')\n",
        "parser.add_argument('-c','--crop', type=int, default=None, help='by default, do not crop. specify crop size')\n",
        "parser.add_argument('--use_cpu', action='store_true', help='uses gpu by default, turn on to use cpu')\n",
        "\n",
        "opt = parser.parse_args()\n",
        "\n",
        "model = resnet50(num_classes=1)\n",
        "state_dict = torch.load(opt.model_path, map_location='cpu')\n",
        "model.load_state_dict(state_dict['model'])\n",
        "if(not opt.use_cpu):\n",
        "  model.cuda()\n",
        "model.eval()\n",
        "\n",
        "# Transform\n",
        "trans_init = []\n",
        "if(opt.crop is not None):\n",
        "  trans_init = [transforms.CenterCrop(opt.crop),]\n",
        "  print('Cropping to [%i]'%opt.crop)\n",
        "else:\n",
        "  print('Not cropping')\n",
        "trans = transforms.Compose(trans_init + [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "img = trans(Image.open('slika1.png').convert('RGB')) \n",
        "\n",
        "with torch.no_grad():\n",
        "    in_tens = img.unsqueeze(0)\n",
        "    if(not opt.use_cpu):\n",
        "    \tin_tens = in_tens.cuda()\n",
        "    prob = model(in_tens).sigmoid().item()\n",
        "\n",
        "print('probability of being fake image: {:.2f}%'.format(prob * 100))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "4aNIcshlSpbf",
        "outputId": "b1884649-99e8-4133-9576-5fefe7cdbd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2fe5d1efecc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2d34205184db>\u001b[0m in \u001b[0;36mresnet50\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpretrained\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrained\u001b[0m \u001b[0mon\u001b[0m \u001b[0mImageNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'resnet50'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ResNet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOSeYEEfoe1C",
        "outputId": "ebf9341d-83f4-4035-9d4b-4ea357e82626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from PIL import Image\n",
        "from networks.resnet import resnet50\n",
        "import glob\n",
        "\n",
        "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "parser.add_argument('-f','--file', default='examples_realfakedir')\n",
        "parser.add_argument('-m','--model_path', type=str, default='weights/blur_jpg_prob0.5.pth')\n",
        "parser.add_argument('-c','--crop', type=int, default=None, help='by default, do not crop. specify crop size')\n",
        "parser.add_argument('--use_cpu', action='store_true', help='uses gpu by default, turn on to use cpu')\n",
        "\n",
        "opt = parser.parse_args()\n",
        "\n",
        "model = resnet50(num_classes=1)\n",
        "state_dict = torch.load(opt.model_path, map_location='cpu')\n",
        "model.load_state_dict(state_dict['model'])\n",
        "if(not opt.use_cpu):\n",
        "  model.cuda()\n",
        "model.eval()\n",
        "\n",
        "# Transform\n",
        "trans_init = []\n",
        "if(opt.crop is not None):\n",
        "  trans_init = [transforms.CenterCrop(opt.crop),]\n",
        "  print('Cropping to [%i]'%opt.crop)\n",
        "else:\n",
        "  print('Not cropping')\n",
        "trans = transforms.Compose(trans_init + [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "imgdir = '/content/gdrive/MyDrive/project_folder/CNN_synth_testset/biggan/0_real/'\n",
        "\n",
        "ext = ['png', 'jpg']\n",
        "\n",
        "files = []\n",
        "[files.extend(glob.glob(imgdir + '*.' + e)) for e in ext]\n",
        "\n",
        "print(len(imgdir))\n",
        "\n",
        "def check(path):\n",
        "\n",
        "  img = trans(Image.open(path).convert('RGB'))\n",
        "\n",
        "  with torch.no_grad():\n",
        "      in_tens = img.unsqueeze(0)\n",
        "      if(not opt.use_cpu):\n",
        "    \t  in_tens = in_tens.cuda()\n",
        "      prob = model(in_tens).sigmoid().item()\n",
        "\n",
        "  print('probability of being fake: {:.2f}%'.format(prob * 100))\n",
        "\n",
        "for file in files:\n",
        "  check(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t9ytB1nevQa",
        "outputId": "1b1e603d-3a0d-4464-fc2b-dbddd5ba98df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not cropping\n",
            "71\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 3.35%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.06%\n",
            "probability of being fake: 1.40%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.30%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.07%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.11%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.26%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.84%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 98.63%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 79.21%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.22%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.20%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.86%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.52%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 13.24%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.12%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.05%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.05%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 19.79%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.14%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.41%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.28%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 3.78%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.09%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.29%\n",
            "probability of being fake: 86.91%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 22.69%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.31%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.49%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.12%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 97.44%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 4.38%\n",
            "probability of being fake: 95.78%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.23%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.38%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 51.48%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 11.52%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.05%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 26.88%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 4.16%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.17%\n",
            "probability of being fake: 99.97%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.55%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 94.65%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.05%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.54%\n",
            "probability of being fake: 9.35%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 3.97%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.07%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 4.69%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.18%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 84.13%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.26%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.09%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.06%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.26%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.13%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.29%\n",
            "probability of being fake: 0.22%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.35%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.62%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.68%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 34.79%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 73.97%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 55.14%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.65%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 17.30%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 33.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.13%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 3.87%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.13%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.18%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 85.43%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.53%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.09%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.50%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.85%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.27%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.57%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.05%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 24.15%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.60%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.32%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.55%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.32%\n",
            "probability of being fake: 22.55%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 48.13%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.32%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.03%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 1.42%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 38.70%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 16.41%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.34%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.08%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 2.30%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.33%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 20.36%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.04%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 49.29%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.05%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.06%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.02%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.57%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.72%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.01%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 100.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.06%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 70.77%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 94.50%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n",
            "probability of being fake: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMhXAkIiYchT",
        "outputId": "675463ef-3be1-4e0f-93ea-958776eba13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 31.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 41.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 32.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "os.chdir('/content/gdrive/MyDrive/project_folder/CNNDetection')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4D2llBQmKb1M",
        "outputId": "67d83e50-37a5-4def-997c-a2313e1ac740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/.shortcut-targets-by-id/1hJv0qBvgMO6MUfTgLSXQA0guuUrUM98B/project_folder/CNNDetection'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from random import random, choice\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def dataset_folder(opt, root):\n",
        "    if opt.mode == 'binary':\n",
        "        return binary_dataset(opt, root)\n",
        "    if opt.mode == 'filename':\n",
        "        return FileNameDataset(opt, root)\n",
        "    raise ValueError('opt.mode needs to be binary or filename.')\n",
        "\n",
        "\n",
        "def binary_dataset(opt, root):\n",
        "    if opt.isTrain:\n",
        "        crop_func = transforms.RandomCrop(opt.cropSize)\n",
        "    elif opt.no_crop:\n",
        "        crop_func = transforms.Lambda(lambda img: img)\n",
        "    else:\n",
        "        crop_func = transforms.CenterCrop(opt.cropSize)\n",
        "\n",
        "    if opt.isTrain and not opt.no_flip:\n",
        "        flip_func = transforms.RandomHorizontalFlip()\n",
        "    else:\n",
        "        flip_func = transforms.Lambda(lambda img: img)\n",
        "    if not opt.isTrain and opt.no_resize:\n",
        "        rz_func = transforms.Lambda(lambda img: img)\n",
        "    else:\n",
        "        rz_func = transforms.Lambda(lambda img: custom_resize(img, opt))\n",
        "\n",
        "    dset = datasets.ImageFolder(\n",
        "            root,\n",
        "            transforms.Compose([\n",
        "                rz_func,\n",
        "                transforms.Lambda(lambda img: data_augment(img, opt)),\n",
        "                crop_func,\n",
        "                flip_func,\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ]))\n",
        "    return dset\n",
        "\n",
        "\n",
        "class FileNameDataset(datasets.ImageFolder):\n",
        "    def name(self):\n",
        "        return 'FileNameDataset'\n",
        "\n",
        "    def __init__(self, opt, root):\n",
        "        self.opt = opt\n",
        "        super().__init__(root)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Loading sample\n",
        "        path, target = self.samples[index]\n",
        "        return path\n",
        "\n",
        "\n",
        "def data_augment(img, opt):\n",
        "    img = np.array(img)\n",
        "\n",
        "    if random() < opt.blur_prob:\n",
        "        sig = sample_continuous(opt.blur_sig)\n",
        "        gaussian_blur(img, sig)\n",
        "\n",
        "    if random() < opt.jpg_prob:\n",
        "        method = sample_discrete(opt.jpg_method)\n",
        "        qual = sample_discrete(opt.jpg_qual)\n",
        "        img = jpeg_from_key(img, qual, method)\n",
        "\n",
        "    return Image.fromarray(img)\n",
        "\n",
        "\n",
        "def sample_continuous(s):\n",
        "    if len(s) == 1:\n",
        "        return s[0]\n",
        "    if len(s) == 2:\n",
        "        rg = s[1] - s[0]\n",
        "        return random() * rg + s[0]\n",
        "    raise ValueError(\"Length of iterable s should be 1 or 2.\")\n",
        "\n",
        "\n",
        "def sample_discrete(s):\n",
        "    if len(s) == 1:\n",
        "        return s[0]\n",
        "    return choice(s)\n",
        "\n",
        "\n",
        "def gaussian_blur(img, sigma):\n",
        "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
        "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
        "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
        "\n",
        "\n",
        "def cv2_jpg(img, compress_val):\n",
        "    img_cv2 = img[:,:,::-1]\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
        "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
        "    decimg = cv2.imdecode(encimg, 1)\n",
        "    return decimg[:,:,::-1]\n",
        "\n",
        "\n",
        "def pil_jpg(img, compress_val):\n",
        "    out = BytesIO()\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(out, format='jpeg', quality=compress_val)\n",
        "    img = Image.open(out)\n",
        "    # load from memory before ByteIO closes\n",
        "    img = np.array(img)\n",
        "    out.close()\n",
        "    return img\n",
        "\n",
        "\n",
        "jpeg_dict = {'cv2': cv2_jpg, 'pil': pil_jpg}\n",
        "def jpeg_from_key(img, compress_val, key):\n",
        "    method = jpeg_dict[key]\n",
        "    return method(img, compress_val)\n",
        "\n",
        "\n",
        "rz_dict = {'bilinear': Image.BILINEAR,\n",
        "           'bicubic': Image.BICUBIC,\n",
        "           'lanczos': Image.LANCZOS,\n",
        "           'nearest': Image.NEAREST}\n",
        "def custom_resize(img, opt):\n",
        "    interp = sample_discrete(opt.rz_interp)\n",
        "    return TF.resize(img, opt.loadSize, interpolation=rz_dict[interp])"
      ],
      "metadata": {
        "id": "Pe3JMkceXyCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash dataset/test/download_testset.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K53wCMiWKyzI",
        "outputId": "35b5762c-3a76-4610-ef3f-c6c23bc81648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2225    0  2225    0     0  14262      0 --:--:-- --:--:-- --:--:-- 14262\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   546  100   546    0     0   3433      0 --:--:-- --:--:-- --:--:--  3433\n",
            "100  114k    0  114k    0     0   462k      0 --:--:-- --:--:-- --:--:--  462k\n",
            "Archive:  testset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of testset.zip or\n",
            "        testset.zip.zip, and cannot find testset.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash dataset/train/download_trainset.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4u1BXrTjAWC",
        "outputId": "cb72795f-5c5f-4bf3-be27-961b7f46c53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1993    0  1993    0     0  11454      0 --:--:-- --:--:-- --:--:-- 11454\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   545  100   545    0     0   3538      0 --:--:-- --:--:-- --:--:--  3538\n",
            "100  114k    0  114k    0     0   463k      0 --:--:-- --:--:-- --:--:--  463k\n",
            "Archive:  trainset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of trainset.zip or\n",
            "        trainset.zip.zip, and cannot find trainset.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=1, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.score_max = -np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, score, model):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(score, model)\n",
        "        elif score < self.best_score - self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(score, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, score, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation accuracy increased ({self.score_max:.6f} --> {score:.6f}).  Saving model ...')\n",
        "        model.save_networks('best')\n",
        "        self.score_max = score"
      ],
      "metadata": {
        "id": "QEyt7Li7X_N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from util import mkdir\n",
        "\n",
        "\n",
        "# directory to store the results\n",
        "results_dir = './results/'\n",
        "mkdir(results_dir)\n",
        "\n",
        "# root to the testsets\n",
        "dataroot = './dataset/test/'\n",
        "\n",
        "# list of synthesis algorithms\n",
        "vals = ['progan', 'stylegan', 'biggan', 'cyclegan', 'stargan', 'gaugan',\n",
        "        'crn', 'imle', 'seeingdark', 'san', 'deepfake', 'stylegan2', 'whichfaceisreal']\n",
        "\n",
        "# indicates if corresponding testset has multiple classes\n",
        "multiclass = [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "\n",
        "# model\n",
        "model_path = 'weights/blur_jpg_prob0.5.pth'"
      ],
      "metadata": {
        "id": "PPPVRWFvYM-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/gdrive/MyDrive/project_folder/CNNDetection/weights/blur_jpg_prob0.5.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2IdRPR-plZ6f",
        "outputId": "fc504a56-b9de-4ce9-f7df-1cb8bd5d9ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8c46a83a-1ea5-4877-83ba-d6e81dc4670e\", \"blur_jpg_prob0.5.pth\", 282442597)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import cv2\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from random import random, choice\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.parallel\n",
        "import torch.nn.functional as F\n",
        "from IPython import embed\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import functools\n",
        "import argparse\n",
        "\n",
        "# from pix2pix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "import os \n",
        "\n",
        "\n",
        "\n",
        "class BaseOptions():\n",
        "    def __init__(self):\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        parser.add_argument('--mode', default='binary')\n",
        "        parser.add_argument('--arch', type=str, default='res50', help='architecture for binary classification')\n",
        "\n",
        "        # data augmentation\n",
        "        parser.add_argument('--rz_interp', default='bilinear')\n",
        "        parser.add_argument('--blur_prob', type=float, default=0)\n",
        "        parser.add_argument('--blur_sig', default='0.5')\n",
        "        parser.add_argument('--jpg_prob', type=float, default=0)\n",
        "        parser.add_argument('--jpg_method', default='cv2')\n",
        "        parser.add_argument('--jpg_qual', default='75')\n",
        "\n",
        "        parser.add_argument('--dataroot', default='./dataset/', help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n",
        "        parser.add_argument('--classes', default='', help='image classes to train on')\n",
        "        parser.add_argument('--class_bal', action='store_true')\n",
        "        parser.add_argument('--batch_size', type=int, default=64, help='input batch size')\n",
        "        parser.add_argument('--loadSize', type=int, default=256, help='scale images to this size')\n",
        "        parser.add_argument('--cropSize', type=int, default=224, help='then crop to this size')\n",
        "        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
        "        parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
        "        parser.add_argument('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
        "        parser.add_argument('--num_threads', default=4, type=int, help='# threads for loading data')\n",
        "        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
        "        parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
        "        parser.add_argument('--resize_or_crop', type=str, default='scale_and_crop', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop|none]')\n",
        "        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
        "        parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
        "        parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
        "        parser.add_argument('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{loadSize}')\n",
        "        self.initialized = True\n",
        "        return parser\n",
        "\n",
        "    def gather_options(self):\n",
        "        # initialize parser with basic options\n",
        "        if not self.initialized:\n",
        "            parser = argparse.ArgumentParser(\n",
        "                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "            parser = self.initialize(parser)\n",
        "\n",
        "        # get the basic options\n",
        "        opt, _ = parser.parse_known_args()\n",
        "        self.parser = parser\n",
        "\n",
        "        return parser.parse_args()\n",
        "\n",
        "    def print_options(self, opt):\n",
        "        message = ''\n",
        "        message += '----------------- Options ---------------\\n'\n",
        "        for k, v in sorted(vars(opt).items()):\n",
        "            comment = ''\n",
        "            default = self.parser.get_default(k)\n",
        "            if v != default:\n",
        "                comment = '\\t[default: %s]' % str(default)\n",
        "            message += '{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment)\n",
        "        message += '----------------- End -------------------'\n",
        "        print(message)\n",
        "\n",
        "        # save to the disk\n",
        "        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "        util.mkdirs(expr_dir)\n",
        "        file_name = os.path.join(expr_dir, 'opt.txt')\n",
        "        with open(file_name, 'wt') as opt_file:\n",
        "            opt_file.write(message)\n",
        "            opt_file.write('\\n')\n",
        "\n",
        "    def parse(self, print_options=True):\n",
        "\n",
        "        opt = self.gather_options()\n",
        "        opt.isTrain = self.isTrain   # train or test\n",
        "\n",
        "        # process opt.suffix\n",
        "        if opt.suffix:\n",
        "            suffix = ('_' + opt.suffix.format(**vars(opt))) if opt.suffix != '' else ''\n",
        "            opt.name = opt.name + suffix\n",
        "\n",
        "        if print_options:\n",
        "            self.print_options(opt)\n",
        "\n",
        "        # set gpu ids\n",
        "        str_ids = opt.gpu_ids.split(',')\n",
        "        opt.gpu_ids = []\n",
        "        for str_id in str_ids:\n",
        "            id = int(str_id)\n",
        "            if id >= 0:\n",
        "                opt.gpu_ids.append(id)\n",
        "        if len(opt.gpu_ids) > 0:\n",
        "            torch.cuda.set_device(opt.gpu_ids[0])\n",
        "\n",
        "        # additional\n",
        "        opt.classes = opt.classes.split(',')\n",
        "        opt.rz_interp = opt.rz_interp.split(',')\n",
        "        opt.blur_sig = [float(s) for s in opt.blur_sig.split(',')]\n",
        "        opt.jpg_method = opt.jpg_method.split(',')\n",
        "        opt.jpg_qual = [int(s) for s in opt.jpg_qual.split(',')]\n",
        "        if len(opt.jpg_qual) == 2:\n",
        "            opt.jpg_qual = list(range(opt.jpg_qual[0], opt.jpg_qual[1] + 1))\n",
        "        elif len(opt.jpg_qual) > 2:\n",
        "            raise ValueError(\"Shouldn't have more than 2 values for --jpg_qual.\")\n",
        "\n",
        "        self.opt = opt\n",
        "        return self.opt\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.total_steps = 0\n",
        "        self.isTrain = opt.isTrain\n",
        "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "        self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')\n",
        "\n",
        "    def save_networks(self, epoch):\n",
        "        save_filename = 'model_epoch_%s.pth' % epoch\n",
        "        save_path = os.path.join(self.save_dir, save_filename)\n",
        "\n",
        "        # serialize model and optimizer to dict\n",
        "        state_dict = {\n",
        "            'model': self.model.state_dict(),\n",
        "            'optimizer' : self.optimizer.state_dict(),\n",
        "            'total_steps' : self.total_steps,\n",
        "        }\n",
        "\n",
        "        torch.save(state_dict, save_path)\n",
        "\n",
        "    # load models from the disk\n",
        "    def load_networks(self, epoch):\n",
        "        load_filename = 'model_epoch_%s.pth' % epoch\n",
        "        load_path = os.path.join(self.save_dir, load_filename)\n",
        "\n",
        "        print('loading the model from %s' % load_path)\n",
        "        # if you are using PyTorch newer than 0.4 (e.g., built from\n",
        "        # GitHub source), you can remove str() on self.device\n",
        "        state_dict = torch.load(load_path, map_location=self.device)\n",
        "        if hasattr(state_dict, '_metadata'):\n",
        "            del state_dict._metadata\n",
        "\n",
        "        self.model.load_state_dict(state_dict['model'])\n",
        "        self.total_steps = state_dict['total_steps']\n",
        "\n",
        "        if self.isTrain and not self.opt.new_optim:\n",
        "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
        "            ### move optimizer state to GPU\n",
        "            for state in self.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.to(self.device)\n",
        "\n",
        "            for g in self.optimizer.param_groups:\n",
        "                g['lr'] = self.opt.lr\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n",
        "\n",
        "    def test(self):\n",
        "        with torch.no_grad():\n",
        "            self.forward()\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)\n",
        "class TestOptions(BaseOptions):\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)\n",
        "        parser.add_argument('--model_path')\n",
        "        parser.add_argument('--no_resize', action='store_true')\n",
        "        parser.add_argument('--no_crop', action='store_true')\n",
        "        parser.add_argument('--eval', action='store_true', help='use eval mode during test time.')\n",
        "\n",
        "        self.isTrain = False\n",
        "        return parser\n",
        "\n",
        "class TrainOptions(BaseOptions):\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)\n",
        "        parser.add_argument('--earlystop_epoch', type=int, default=5)\n",
        "        parser.add_argument('--data_aug', action='store_true', help='if specified, perform additional data augmentation (photometric, blurring, jpegging)')\n",
        "        parser.add_argument('--optim', type=str, default='adam', help='optim to use [sgd, adam]')\n",
        "        parser.add_argument('--new_optim', action='store_true', help='new optimizer instead of loading the optim state')\n",
        "        parser.add_argument('--loss_freq', type=int, default=400, help='frequency of showing loss on tensorboard')\n",
        "        parser.add_argument('--save_latest_freq', type=int, default=2000, help='frequency of saving the latest results')\n",
        "        parser.add_argument('--save_epoch_freq', type=int, default=20, help='frequency of saving checkpoints at the end of epochs')\n",
        "        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
        "        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
        "        parser.add_argument('--last_epoch', type=int, default=-1, help='starting epoch count for scheduler intialization')\n",
        "        parser.add_argument('--train_split', type=str, default='train', help='train, val, test, etc')\n",
        "        parser.add_argument('--val_split', type=str, default='val', help='train, val, test, etc')\n",
        "        parser.add_argument('--niter', type=int, default=10000, help='# of iter at starting learning rate')\n",
        "        parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')\n",
        "        parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\n",
        "\n",
        "        self.isTrain = True\n",
        "        return parser\n",
        "#util\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def unnormalize(tens, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    # assume tensor of shape NxCxHxW\n",
        "    return tens * torch.Tensor(std)[None, :, None, None] + torch.Tensor(\n",
        "        mean)[None, :, None, None]\n",
        "\n",
        "class Trainer(BaseModel):\n",
        "    def name(self):\n",
        "        return 'Trainer'\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(Trainer, self).__init__(opt)\n",
        "\n",
        "        if self.isTrain and not opt.continue_train:\n",
        "            self.model = resnet50(pretrained=True)\n",
        "            self.model.fc = nn.Linear(2048, 1)\n",
        "            torch.nn.init.normal_(self.model.fc.weight.data, 0.0, opt.init_gain)\n",
        "\n",
        "        if not self.isTrain or opt.continue_train:\n",
        "            self.model = resnet50(num_classes=1)\n",
        "\n",
        "        if self.isTrain:\n",
        "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "            # initialize optimizers\n",
        "            if opt.optim == 'adam':\n",
        "                self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
        "                                                  lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            elif opt.optim == 'sgd':\n",
        "                self.optimizer = torch.optim.SGD(self.model.parameters(),\n",
        "                                                 lr=opt.lr, momentum=0.0, weight_decay=0)\n",
        "            else:\n",
        "                raise ValueError(\"optim should be [adam, sgd]\")\n",
        "\n",
        "        if not self.isTrain or opt.continue_train:\n",
        "            self.load_networks(opt.epoch)\n",
        "        self.model.to(opt.gpu_ids[0])\n",
        "\n",
        "\n",
        "    def adjust_learning_rate(self, min_lr=1e-6):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] /= 10.\n",
        "            if param_group['lr'] < min_lr:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def set_input(self, input):\n",
        "        self.input = input[0].to(self.device)\n",
        "        self.label = input[1].to(self.device).float()\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        self.output = self.model(self.input)\n",
        "\n",
        "    def get_loss(self):\n",
        "        return self.loss_fn(self.output.squeeze(1), self.label)\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        self.forward()\n",
        "        self.loss = self.loss_fn(self.output.squeeze(1), self.label)\n",
        "        self.optimizer.zero_grad()\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "     'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "     'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "     'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "     'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "     'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        " }\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                 padding=1, groups=groups, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1:\n",
        "            raise ValueError('BasicBlock only supports groups=1')\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        if(stride==1):\n",
        "            self.conv2 = conv3x3(planes,planes)\n",
        "        else:\n",
        "            self.conv2 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),\n",
        "                conv3x3(planes, planes),)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, groups) # stride moved\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        if(stride==1):\n",
        "            self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        else:\n",
        "            self.conv3 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),\n",
        "                conv1x1(planes, planes * self.expansion))\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, norm_layer=None, filter_size=1, pool_only=True):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        planes = [int(width_per_group * groups * 2 ** i) for i in range(4)]\n",
        "        self.inplanes = planes[0]\n",
        "\n",
        "        if(pool_only):\n",
        "            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=1, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(planes[0])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if(pool_only):\n",
        "            self.maxpool = nn.Sequential(*[nn.MaxPool2d(kernel_size=2, stride=1), \n",
        "                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])\n",
        "        else:\n",
        "            self.maxpool = nn.Sequential(*[Downsample(filt_size=filter_size, stride=2, channels=planes[0]), \n",
        "                nn.MaxPool2d(kernel_size=2, stride=1), \n",
        "                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])\n",
        "\n",
        "        self.layer1 = self._make_layer(block, planes[0], layers[0], groups=groups, norm_layer=norm_layer)\n",
        "        self.layer2 = self._make_layer(block, planes[1], layers[1], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n",
        "        self.layer3 = self._make_layer(block, planes[2], layers[2], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n",
        "        self.layer4 = self._make_layer(block, planes[3], layers[3], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(planes[3] * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if(m.in_channels!=m.out_channels or m.out_channels!=m.groups or m.bias is not None):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                else:\n",
        "                    print('Not initializing')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, groups=1, norm_layer=None, filter_size=1):\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "                 conv1x1(self.inplanes, planes * block.expansion, stride, filter_size=filter_size),\n",
        "                 norm_layer(planes * block.expansion),\n",
        "             )\n",
        "\n",
        "          downsample = [Downsample(filt_size=filter_size, stride=stride, channels=self.inplanes),] if(stride !=1) else []\n",
        "          downsample += [conv1x1(self.inplanes, planes * block.expansion, 1),\n",
        "                norm_layer(planes * block.expansion)]\n",
        "            # print(downsample)\n",
        "          downsample = nn.Sequential(*downsample)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, groups, norm_layer, filter_size=filter_size))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=groups, norm_layer=norm_layer, filter_size=filter_size))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnext50_32x4d(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], groups=4, width_per_group=32, filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "         model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnext101_32x8d(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], groups=8, width_per_group=32, filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    # if pretrained:\n",
        "    #     model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)), int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2))]\n",
        "        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride-1)/2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        if(self.filt_size==1):\n",
        "            a = np.array([1.,])\n",
        "        elif(self.filt_size==2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size==3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size==4):    \n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size==5):    \n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size==6):    \n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size==7):    \n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a[:,None]*a[None,:])\n",
        "        filt = filt/torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None,None,:,:].repeat((self.channels,1,1,1)))\n",
        "\n",
        "        self.pad = get_pad_layer(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size==1):\n",
        "            if(self.pad_off==0):\n",
        "                return inp[:,:,::self.stride,::self.stride]    \n",
        "            else:\n",
        "                return self.pad(inp)[:,:,::self.stride,::self.stride]\n",
        "        else:\n",
        "            return F.conv2d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "def get_pad_layer(pad_type):\n",
        "    if(pad_type in ['refl','reflect']):\n",
        "        PadLayer = nn.ReflectionPad2d\n",
        "    elif(pad_type in ['repl','replicate']):\n",
        "        PadLayer = nn.ReplicationPad2d\n",
        "    elif(pad_type=='zero'):\n",
        "        PadLayer = nn.ZeroPad2d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized'%pad_type)\n",
        "    return PadLayer\n",
        "\n",
        "\n",
        "class Downsample1D(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample1D, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2))]\n",
        "        self.pad_sizes = [pad_size + pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride - 1) / 2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        if(self.filt_size == 1):\n",
        "            a = np.array([1., ])\n",
        "        elif(self.filt_size == 2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size == 3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size == 4):\n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size == 5):\n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size == 6):\n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size == 7):\n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a)\n",
        "        filt = filt / torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None, None, :].repeat((self.channels, 1, 1)))\n",
        "\n",
        "        self.pad = get_pad_layer_1d(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size == 1):\n",
        "            if(self.pad_off == 0):\n",
        "                return inp[:, :, ::self.stride]\n",
        "            else:\n",
        "                return self.pad(inp)[:, :, ::self.stride]\n",
        "        else:\n",
        "            return F.conv1d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "\n",
        "def get_pad_layer_1d(pad_type):\n",
        "    if(pad_type in ['refl', 'reflect']):\n",
        "        PadLayer = nn.ReflectionPad1d\n",
        "    elif(pad_type in ['repl', 'replicate']):\n",
        "        PadLayer = nn.ReplicationPad1d\n",
        "    elif(pad_type == 'zero'):\n",
        "        PadLayer = nn.ZeroPad1d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized' % pad_type)\n",
        "    return PadLayer\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def dataset_folder(opt, root):\n",
        "    if opt.mode == 'binary':\n",
        "        return binary_dataset(opt, root)\n",
        "    if opt.mode == 'filename':\n",
        "        return FileNameDataset(opt, root)\n",
        "    raise ValueError('opt.mode needs to be binary or filename.')\n",
        "\n",
        "\n",
        "def binary_dataset(opt, root):\n",
        "    if opt.isTrain:\n",
        "        crop_func = transforms.RandomCrop(opt.cropSize)\n",
        "    elif opt.no_crop:\n",
        "        crop_func = transforms.Lambda(lambda img: img)\n",
        "    else:\n",
        "        crop_func = transforms.CenterCrop(opt.cropSize)\n",
        "\n",
        "    if opt.isTrain and not opt.no_flip:\n",
        "        flip_func = transforms.RandomHorizontalFlip()\n",
        "    else:\n",
        "        flip_func = transforms.Lambda(lambda img: img)\n",
        "    if not opt.isTrain and opt.no_resize:\n",
        "        rz_func = transforms.Lambda(lambda img: img)\n",
        "    else:\n",
        "        rz_func = transforms.Lambda(lambda img: custom_resize(img, opt))\n",
        "\n",
        "    dset = datasets.ImageFolder(\n",
        "            root,\n",
        "            transforms.Compose([\n",
        "                rz_func,\n",
        "                transforms.Lambda(lambda img: data_augment(img, opt)),\n",
        "                crop_func,\n",
        "                flip_func,\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ]))\n",
        "    return dset\n",
        "\n",
        "\n",
        "class FileNameDataset(datasets.ImageFolder):\n",
        "    def name(self):\n",
        "        return 'FileNameDataset'\n",
        "\n",
        "    def __init__(self, opt, root):\n",
        "        self.opt = opt\n",
        "        super().__init__(root)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Loading sample\n",
        "        path, target = self.samples[index]\n",
        "        return path\n",
        "\n",
        "\n",
        "def data_augment(img, opt):\n",
        "    img = np.array(img)\n",
        "\n",
        "    if random() < opt.blur_prob:\n",
        "        sig = sample_continuous(opt.blur_sig)\n",
        "        gaussian_blur(img, sig)\n",
        "\n",
        "    if random() < opt.jpg_prob:\n",
        "        method = sample_discrete(opt.jpg_method)\n",
        "        qual = sample_discrete(opt.jpg_qual)\n",
        "        img = jpeg_from_key(img, qual, method)\n",
        "\n",
        "    return Image.fromarray(img)\n",
        "\n",
        "\n",
        "def sample_continuous(s):\n",
        "    if len(s) == 1:\n",
        "        return s[0]\n",
        "    if len(s) == 2:\n",
        "        rg = s[1] - s[0]\n",
        "        return random() * rg + s[0]\n",
        "    raise ValueError(\"Length of iterable s should be 1 or 2.\")\n",
        "\n",
        "\n",
        "def sample_discrete(s):\n",
        "    if len(s) == 1:\n",
        "        return s[0]\n",
        "    return choice(s)\n",
        "\n",
        "\n",
        "def gaussian_blur(img, sigma):\n",
        "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
        "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
        "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
        "\n",
        "\n",
        "def cv2_jpg(img, compress_val):\n",
        "    img_cv2 = img[:,:,::-1]\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
        "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
        "    decimg = cv2.imdecode(encimg, 1)\n",
        "    return decimg[:,:,::-1]\n",
        "\n",
        "\n",
        "def pil_jpg(img, compress_val):\n",
        "    out = BytesIO()\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(out, format='jpeg', quality=compress_val)\n",
        "    img = Image.open(out)\n",
        "    # load from memory before ByteIO closes\n",
        "    img = np.array(img)\n",
        "    out.close()\n",
        "    return img\n",
        "\n",
        "\n",
        "jpeg_dict = {'cv2': cv2_jpg, 'pil': pil_jpg}\n",
        "def jpeg_from_key(img, compress_val, key):\n",
        "    method = jpeg_dict[key]\n",
        "    return method(img, compress_val)\n",
        "\n",
        "\n",
        "rz_dict = {'bilinear': Image.BILINEAR,\n",
        "           'bicubic': Image.BICUBIC,\n",
        "           'lanczos': Image.LANCZOS,\n",
        "           'nearest': Image.NEAREST}\n",
        "def custom_resize(img, opt):\n",
        "    interp = sample_discrete(opt.rz_interp)\n",
        "    return TF.resize(img, opt.loadSize, interpolation=rz_dict[interp])\n",
        "\n",
        "\n",
        "def get_dataset(opt):\n",
        "    dset_lst = []\n",
        "    for cls in opt.classes:\n",
        "        root = opt.dataroot + '/' + cls\n",
        "        dset = dataset_folder(opt, root)\n",
        "        dset_lst.append(dset)\n",
        "    return torch.utils.data.ConcatDataset(dset_lst)\n",
        "\n",
        "\n",
        "def get_bal_sampler(dataset):\n",
        "    targets = []\n",
        "    for d in dataset.datasets:\n",
        "        targets.extend(d.targets)\n",
        "\n",
        "    ratio = np.bincount(targets)\n",
        "    w = 1. / torch.tensor(ratio, dtype=torch.float)\n",
        "    sample_weights = w[targets]\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights,\n",
        "                                    num_samples=len(sample_weights))\n",
        "    return sampler\n",
        "\n",
        "\n",
        "def create_dataloader(opt):\n",
        "    shuffle = not opt.serial_batches if (opt.isTrain and not opt.class_bal) else False\n",
        "    dataset = get_dataset(opt)\n",
        "    sampler = get_bal_sampler(dataset) if opt.class_bal else None\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                              batch_size=opt.batch_size,\n",
        "                                              shuffle=shuffle,\n",
        "                                              sampler=sampler,\n",
        "                                              num_workers=int(opt.num_threads))\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "TrL0mMpbS4_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import cv2\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from random import random, choice\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.parallel\n",
        "import torch.nn.functional as F\n",
        "from IPython import embed\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import functools\n",
        "import argparse\n",
        "# from pix2pix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "import os \n",
        "\n",
        "\n",
        "\n",
        "class BaseOptions():\n",
        "    def __init__(self):\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        parser.add_argument('--mode', default='binary')\n",
        "        parser.add_argument('--arch', type=str, default='res50', help='architecture for binary classification')\n",
        "\n",
        "        # data augmentation\n",
        "        parser.add_argument('--rz_interp', default='bilinear')\n",
        "        parser.add_argument('--blur_prob', type=float, default=0)\n",
        "        parser.add_argument('--blur_sig', default='0.5')\n",
        "        parser.add_argument('--jpg_prob', type=float, default=0)\n",
        "        parser.add_argument('--jpg_method', default='cv2')\n",
        "        parser.add_argument('--jpg_qual', default='75')\n",
        "\n",
        "        parser.add_argument('--dataroot', default='./dataset/', help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n",
        "        parser.add_argument('--classes', default='', help='image classes to train on')\n",
        "        parser.add_argument('--class_bal', action='store_true')\n",
        "        parser.add_argument('--batch_size', type=int, default=64, help='input batch size')\n",
        "        parser.add_argument('--loadSize', type=int, default=256, help='scale images to this size')\n",
        "        parser.add_argument('--cropSize', type=int, default=224, help='then crop to this size')\n",
        "        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
        "        parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n",
        "        parser.add_argument('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
        "        parser.add_argument('--num_threads', default=4, type=int, help='# threads for loading data')\n",
        "        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
        "        parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
        "        parser.add_argument('--resize_or_crop', type=str, default='scale_and_crop', help='scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop|none]')\n",
        "        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n",
        "        parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
        "        parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n",
        "        parser.add_argument('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{loadSize}')\n",
        "        self.initialized = True\n",
        "        return parser\n",
        "\n",
        "    def gather_options(self):\n",
        "        # initialize parser with basic options\n",
        "        if not self.initialized:\n",
        "            parser = argparse.ArgumentParser(\n",
        "                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "            parser = self.initialize(parser)\n",
        "\n",
        "        # get the basic options\n",
        "        opt, _ = parser.parse_known_args()\n",
        "        self.parser = parser\n",
        "\n",
        "        return parser.parse_args()\n",
        "\n",
        "    def print_options(self, opt):\n",
        "        message = ''\n",
        "        message += '----------------- Options ---------------\\n'\n",
        "        for k, v in sorted(vars(opt).items()):\n",
        "            comment = ''\n",
        "            default = self.parser.get_default(k)\n",
        "            if v != default:\n",
        "                comment = '\\t[default: %s]' % str(default)\n",
        "            message += '{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment)\n",
        "        message += '----------------- End -------------------'\n",
        "        print(message)\n",
        "\n",
        "        # save to the disk\n",
        "        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "        util.mkdirs(expr_dir)\n",
        "        file_name = os.path.join(expr_dir, 'opt.txt')\n",
        "        with open(file_name, 'wt') as opt_file:\n",
        "            opt_file.write(message)\n",
        "            opt_file.write('\\n')\n",
        "\n",
        "    def parse(self, print_options=True):\n",
        "\n",
        "        opt = self.gather_options()\n",
        "        opt.isTrain = self.isTrain   # train or test\n",
        "\n",
        "        # process opt.suffix\n",
        "        if opt.suffix:\n",
        "            suffix = ('_' + opt.suffix.format(**vars(opt))) if opt.suffix != '' else ''\n",
        "            opt.name = opt.name + suffix\n",
        "\n",
        "        if print_options:\n",
        "            self.print_options(opt)\n",
        "\n",
        "        # set gpu ids\n",
        "        str_ids = opt.gpu_ids.split(',')\n",
        "        opt.gpu_ids = []\n",
        "        for str_id in str_ids:\n",
        "            id = int(str_id)\n",
        "            if id >= 0:\n",
        "                opt.gpu_ids.append(id)\n",
        "        if len(opt.gpu_ids) > 0:\n",
        "            torch.cuda.set_device(opt.gpu_ids[0])\n",
        "\n",
        "        # additional\n",
        "        opt.classes = opt.classes.split(',')\n",
        "        opt.rz_interp = opt.rz_interp.split(',')\n",
        "        opt.blur_sig = [float(s) for s in opt.blur_sig.split(',')]\n",
        "        opt.jpg_method = opt.jpg_method.split(',')\n",
        "        opt.jpg_qual = [int(s) for s in opt.jpg_qual.split(',')]\n",
        "        if len(opt.jpg_qual) == 2:\n",
        "            opt.jpg_qual = list(range(opt.jpg_qual[0], opt.jpg_qual[1] + 1))\n",
        "        elif len(opt.jpg_qual) > 2:\n",
        "            raise ValueError(\"Shouldn't have more than 2 values for --jpg_qual.\")\n",
        "\n",
        "        self.opt = opt\n",
        "        return self.opt\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.opt = opt\n",
        "        self.total_steps = 0\n",
        "        self.isTrain = opt.isTrain\n",
        "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "        self.device = torch.device('cuda:{}'.format(opt.gpu_ids[0])) if opt.gpu_ids else torch.device('cpu')\n",
        "\n",
        "    def save_networks(self, epoch):\n",
        "        save_filename = 'model_epoch_%s.pth' % epoch\n",
        "        save_path = os.path.join(self.save_dir, save_filename)\n",
        "\n",
        "        # serialize model and optimizer to dict\n",
        "        state_dict = {\n",
        "            'model': self.model.state_dict(),\n",
        "            'optimizer' : self.optimizer.state_dict(),\n",
        "            'total_steps' : self.total_steps,\n",
        "        }\n",
        "\n",
        "        torch.save(state_dict, save_path)\n",
        "\n",
        "    # load models from the disk\n",
        "    def load_networks(self, epoch):\n",
        "        load_filename = 'model_epoch_%s.pth' % epoch\n",
        "        load_path = os.path.join(self.save_dir, load_filename)\n",
        "\n",
        "        print('loading the model from %s' % load_path)\n",
        "        # if you are using PyTorch newer than 0.4 (e.g., built from\n",
        "        # GitHub source), you can remove str() on self.device\n",
        "        state_dict = torch.load(load_path, map_location=self.device)\n",
        "        if hasattr(state_dict, '_metadata'):\n",
        "            del state_dict._metadata\n",
        "\n",
        "        self.model.load_state_dict(state_dict['model'])\n",
        "        self.total_steps = state_dict['total_steps']\n",
        "\n",
        "        if self.isTrain and not self.opt.new_optim:\n",
        "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
        "            ### move optimizer state to GPU\n",
        "            for state in self.optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = v.to(self.device)\n",
        "\n",
        "            for g in self.optimizer.param_groups:\n",
        "                g['lr'] = self.opt.lr\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n",
        "\n",
        "    def test(self):\n",
        "        with torch.no_grad():\n",
        "            self.forward()\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)\n",
        "class TestOptions(BaseOptions):\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)\n",
        "        parser.add_argument('--model_path')\n",
        "        parser.add_argument('--no_resize', action='store_true')\n",
        "        parser.add_argument('--no_crop', action='store_true')\n",
        "        parser.add_argument('--eval', action='store_true', help='use eval mode during test time.')\n",
        "\n",
        "        self.isTrain = False\n",
        "        return parser\n",
        "\n",
        "class TrainOptions(BaseOptions):\n",
        "    def initialize(self, parser):\n",
        "        parser = BaseOptions.initialize(self, parser)\n",
        "        parser.add_argument('--earlystop_epoch', type=int, default=5)\n",
        "        parser.add_argument('--data_aug', action='store_true', help='if specified, perform additional data augmentation (photometric, blurring, jpegging)')\n",
        "        parser.add_argument('--optim', type=str, default='adam', help='optim to use [sgd, adam]')\n",
        "        parser.add_argument('--new_optim', action='store_true', help='new optimizer instead of loading the optim state')\n",
        "        parser.add_argument('--loss_freq', type=int, default=400, help='frequency of showing loss on tensorboard')\n",
        "        parser.add_argument('--save_latest_freq', type=int, default=2000, help='frequency of saving the latest results')\n",
        "        parser.add_argument('--save_epoch_freq', type=int, default=20, help='frequency of saving checkpoints at the end of epochs')\n",
        "        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
        "        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
        "        parser.add_argument('--last_epoch', type=int, default=-1, help='starting epoch count for scheduler intialization')\n",
        "        parser.add_argument('--train_split', type=str, default='train', help='train, val, test, etc')\n",
        "        parser.add_argument('--val_split', type=str, default='val', help='train, val, test, etc')\n",
        "        parser.add_argument('--niter', type=int, default=10000, help='# of iter at starting learning rate')\n",
        "        parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')\n",
        "        parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\n",
        "\n",
        "        self.isTrain = True\n",
        "        return parser\n",
        "#util\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def unnormalize(tens, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    # assume tensor of shape NxCxHxW\n",
        "    return tens * torch.Tensor(std)[None, :, None, None] + torch.Tensor(\n",
        "        mean)[None, :, None, None]\n",
        "class Trainer(BaseModel):\n",
        "    def name(self):\n",
        "        return 'Trainer'\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super(Trainer, self).__init__(opt)\n",
        "\n",
        "        if self.isTrain and not opt.continue_train:\n",
        "            self.model = resnet50(pretrained=True)\n",
        "            self.model.fc = nn.Linear(2048, 1)\n",
        "            torch.nn.init.normal_(self.model.fc.weight.data, 0.0, opt.init_gain)\n",
        "\n",
        "        if not self.isTrain or opt.continue_train:\n",
        "            self.model = resnet50(num_classes=1)\n",
        "\n",
        "        if self.isTrain:\n",
        "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "            # initialize optimizers\n",
        "            if opt.optim == 'adam':\n",
        "                self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
        "                                                  lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "            elif opt.optim == 'sgd':\n",
        "                self.optimizer = torch.optim.SGD(self.model.parameters(),\n",
        "                                                 lr=opt.lr, momentum=0.0, weight_decay=0)\n",
        "            else:\n",
        "                raise ValueError(\"optim should be [adam, sgd]\")\n",
        "\n",
        "        if not self.isTrain or opt.continue_train:\n",
        "            self.load_networks(opt.epoch)\n",
        "        self.model.to(opt.gpu_ids[0])\n",
        "\n",
        "\n",
        "    def adjust_learning_rate(self, min_lr=1e-6):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] /= 10.\n",
        "            if param_group['lr'] < min_lr:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def set_input(self, input):\n",
        "        self.input = input[0].to(self.device)\n",
        "        self.label = input[1].to(self.device).float()\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        self.output = self.model(self.input)\n",
        "\n",
        "    def get_loss(self):\n",
        "        return self.loss_fn(self.output.squeeze(1), self.label)\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        self.forward()\n",
        "        self.loss = self.loss_fn(self.output.squeeze(1), self.label)\n",
        "        self.optimizer.zero_grad()\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "     'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "     'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "     'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "     'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "     'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        " }\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                 padding=1, groups=groups, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1:\n",
        "            raise ValueError('BasicBlock only supports groups=1')\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        if(stride==1):\n",
        "            self.conv2 = conv3x3(planes,planes)\n",
        "        else:\n",
        "            self.conv2 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),\n",
        "                conv3x3(planes, planes),)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, groups) # stride moved\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        if(stride==1):\n",
        "            self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        else:\n",
        "            self.conv3 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),\n",
        "                conv1x1(planes, planes * self.expansion))\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, norm_layer=None, filter_size=1, pool_only=True):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        planes = [int(width_per_group * groups * 2 ** i) for i in range(4)]\n",
        "        self.inplanes = planes[0]\n",
        "\n",
        "        if(pool_only):\n",
        "            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=1, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(planes[0])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if(pool_only):\n",
        "            self.maxpool = nn.Sequential(*[nn.MaxPool2d(kernel_size=2, stride=1), \n",
        "                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])\n",
        "        else:\n",
        "            self.maxpool = nn.Sequential(*[Downsample(filt_size=filter_size, stride=2, channels=planes[0]), \n",
        "                nn.MaxPool2d(kernel_size=2, stride=1), \n",
        "                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])\n",
        "\n",
        "        self.layer1 = self._make_layer(block, planes[0], layers[0], groups=groups, norm_layer=norm_layer)\n",
        "        self.layer2 = self._make_layer(block, planes[1], layers[1], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n",
        "        self.layer3 = self._make_layer(block, planes[2], layers[2], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n",
        "        self.layer4 = self._make_layer(block, planes[3], layers[3], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(planes[3] * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if(m.in_channels!=m.out_channels or m.out_channels!=m.groups or m.bias is not None):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                else:\n",
        "                    print('Not initializing')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, groups=1, norm_layer=None, filter_size=1):\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "          downsample = nn.Sequential(\n",
        "                 conv1x1(self.inplanes, planes * block.expansion, stride, filter_size=filter_size),\n",
        "                 norm_layer(planes * block.expansion),\n",
        "             )\n",
        "\n",
        "          downsample = [Downsample(filt_size=filter_size, stride=stride, channels=self.inplanes),] if(stride !=1) else []\n",
        "          downsample += [conv1x1(self.inplanes, planes * block.expansion, 1),\n",
        "                norm_layer(planes * block.expansion)]\n",
        "            # print(downsample)\n",
        "          downsample = nn.Sequential(*downsample)\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, groups, norm_layer, filter_size=filter_size))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=groups, norm_layer=norm_layer, filter_size=filter_size))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnext50_32x4d(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], groups=4, width_per_group=32, filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    if pretrained:\n",
        "         model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnext101_32x8d(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], groups=8, width_per_group=32, filter_size=filter_size, pool_only=pool_only, **kwargs)\n",
        "    # if pretrained:\n",
        "    #     model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
        "    return model\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)), int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2))]\n",
        "        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride-1)/2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        if(self.filt_size==1):\n",
        "            a = np.array([1.,])\n",
        "        elif(self.filt_size==2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size==3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size==4):    \n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size==5):    \n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size==6):    \n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size==7):    \n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a[:,None]*a[None,:])\n",
        "        filt = filt/torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None,None,:,:].repeat((self.channels,1,1,1)))\n",
        "\n",
        "        self.pad = get_pad_layer(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size==1):\n",
        "            if(self.pad_off==0):\n",
        "                return inp[:,:,::self.stride,::self.stride]    \n",
        "            else:\n",
        "                return self.pad(inp)[:,:,::self.stride,::self.stride]\n",
        "        else:\n",
        "            return F.conv2d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "def get_pad_layer(pad_type):\n",
        "    if(pad_type in ['refl','reflect']):\n",
        "        PadLayer = nn.ReflectionPad2d\n",
        "    elif(pad_type in ['repl','replicate']):\n",
        "        PadLayer = nn.ReplicationPad2d\n",
        "    elif(pad_type=='zero'):\n",
        "        PadLayer = nn.ZeroPad2d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized'%pad_type)\n",
        "    return PadLayer\n",
        "\n",
        "\n",
        "class Downsample1D(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample1D, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2))]\n",
        "        self.pad_sizes = [pad_size + pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride - 1) / 2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        if(self.filt_size == 1):\n",
        "            a = np.array([1., ])\n",
        "        elif(self.filt_size == 2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size == 3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size == 4):\n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size == 5):\n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size == 6):\n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size == 7):\n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a)\n",
        "        filt = filt / torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None, None, :].repeat((self.channels, 1, 1)))\n",
        "\n",
        "        self.pad = get_pad_layer_1d(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size == 1):\n",
        "            if(self.pad_off == 0):\n",
        "                return inp[:, :, ::self.stride]\n",
        "            else:\n",
        "                return self.pad(inp)[:, :, ::self.stride]\n",
        "        else:\n",
        "            return F.conv1d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "\n",
        "def get_pad_layer_1d(pad_type):\n",
        "    if(pad_type in ['refl', 'reflect']):\n",
        "        PadLayer = nn.ReflectionPad1d\n",
        "    elif(pad_type in ['repl', 'replicate']):\n",
        "        PadLayer = nn.ReplicationPad1d\n",
        "    elif(pad_type == 'zero'):\n",
        "        PadLayer = nn.ZeroPad1d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized' % pad_type)\n",
        "    return PadLayer\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def dataset_folder(opt, root):\n",
        "    if opt.mode == 'binary':\n",
        "        return binary_dataset(opt, root)\n",
        "    if opt.mode == 'filename':\n",
        "        return FileNameDataset(opt, root)\n",
        "    raise ValueError('opt.mode needs to be binary or filename.')\n",
        "\n",
        "\n",
        "def binary_dataset(opt, root):\n",
        "    if opt.isTrain:\n",
        "        crop_func = transforms.RandomCrop(opt.cropSize)\n",
        "    elif opt.no_crop:\n",
        "        crop_func = transforms.Lambda(lambda img: img)\n",
        "    else:\n",
        "        crop_func = transforms.CenterCrop(opt.cropSize)\n",
        "\n",
        "    if opt.isTrain and not opt.no_flip:\n",
        "        flip_func = transforms.RandomHorizontalFlip()\n",
        "    else:\n",
        "        flip_func = transforms.Lambda(lambda img: img)\n",
        "    if not opt.isTrain and opt.no_resize:\n",
        "        rz_func = transforms.Lambda(lambda img: img)\n",
        "    else:\n",
        "        rz_func = transforms.Lambda(lambda img: custom_resize(img, opt))\n",
        "\n",
        "    dset = datasets.ImageFolder(\n",
        "            root,\n",
        "            transforms.Compose([\n",
        "                rz_func,\n",
        "                transforms.Lambda(lambda img: data_augment(img, opt)),\n",
        "                crop_func,\n",
        "                flip_func,\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ]))\n",
        "    return dset\n",
        "\n",
        "\n",
        "class FileNameDataset(datasets.ImageFolder):\n",
        "    def name(self):\n",
        "        return 'FileNameDataset'\n",
        "\n",
        "    def __init__(self, opt, root):\n",
        "        self.opt = opt\n",
        "        super().__init__(root)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Loading sample\n",
        "        path, target = self.samples[index]\n",
        "        return path\n",
        "\n",
        "\n",
        "def data_augment(img, opt):\n",
        "    img = np.array(img)\n",
        "\n",
        "    if random() < opt.blur_prob:\n",
        "        sig = sample_continuous(opt.blur_sig)\n",
        "        gaussian_blur(img, sig)\n",
        "\n",
        "    if random() < opt.jpg_prob:\n",
        "        method = sample_discrete(opt.jpg_method)\n",
        "        qual = sample_discrete(opt.jpg_qual)\n",
        "        img = jpeg_from_key(img, qual, method)\n",
        "\n",
        "    return Image.fromarray(img)\n",
        "\n",
        "\n",
        "def sample_continuous(s):\n",
        "    if len(s) == 1:\n",
        "        return s[0]\n",
        "    if len(s) == 2:\n",
        "        rg = s[1] - s[0]\n",
        "        return random() * rg + s[0]\n",
        "    raise ValueError(\"Length of iterable s should be 1 or 2.\")\n",
        "\n",
        "\n",
        "def sample_discrete(s):\n",
        "    if len(s) == 1:\n",
        "        return s[0]\n",
        "    return choice(s)\n",
        "\n",
        "\n",
        "def gaussian_blur(img, sigma):\n",
        "    gaussian_filter(img[:,:,0], output=img[:,:,0], sigma=sigma)\n",
        "    gaussian_filter(img[:,:,1], output=img[:,:,1], sigma=sigma)\n",
        "    gaussian_filter(img[:,:,2], output=img[:,:,2], sigma=sigma)\n",
        "\n",
        "\n",
        "def cv2_jpg(img, compress_val):\n",
        "    img_cv2 = img[:,:,::-1]\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), compress_val]\n",
        "    result, encimg = cv2.imencode('.jpg', img_cv2, encode_param)\n",
        "    decimg = cv2.imdecode(encimg, 1)\n",
        "    return decimg[:,:,::-1]\n",
        "\n",
        "\n",
        "def pil_jpg(img, compress_val):\n",
        "    out = BytesIO()\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(out, format='jpeg', quality=compress_val)\n",
        "    img = Image.open(out)\n",
        "    # load from memory before ByteIO closes\n",
        "    img = np.array(img)\n",
        "    out.close()\n",
        "    return img\n",
        "\n",
        "\n",
        "jpeg_dict = {'cv2': cv2_jpg, 'pil': pil_jpg}\n",
        "def jpeg_from_key(img, compress_val, key):\n",
        "    method = jpeg_dict[key]\n",
        "    return method(img, compress_val)\n",
        "\n",
        "\n",
        "rz_dict = {'bilinear': Image.BILINEAR,\n",
        "           'bicubic': Image.BICUBIC,\n",
        "           'lanczos': Image.LANCZOS,\n",
        "           'nearest': Image.NEAREST}\n",
        "def custom_resize(img, opt):\n",
        "    interp = sample_discrete(opt.rz_interp)\n",
        "    return TF.resize(img, opt.loadSize, interpolation=rz_dict[interp])\n",
        "\n",
        "\n",
        "def get_dataset(opt):\n",
        "    dset_lst = []\n",
        "    for cls in opt.classes:\n",
        "        root = opt.dataroot + '/' + cls\n",
        "        dset = dataset_folder(opt, root)\n",
        "        dset_lst.append(dset)\n",
        "    return torch.utils.data.ConcatDataset(dset_lst)\n",
        "\n",
        "\n",
        "def get_bal_sampler(dataset):\n",
        "    targets = []\n",
        "    for d in dataset.datasets:\n",
        "        targets.extend(d.targets)\n",
        "\n",
        "    ratio = np.bincount(targets)\n",
        "    w = 1. / torch.tensor(ratio, dtype=torch.float)\n",
        "    sample_weights = w[targets]\n",
        "    sampler = WeightedRandomSampler(weights=sample_weights,\n",
        "                                    num_samples=len(sample_weights))\n",
        "    return sampler\n",
        "\n",
        "\n",
        "def create_dataloader(opt):\n",
        "    shuffle = not opt.serial_batches if (opt.isTrain and not opt.class_bal) else False\n",
        "    dataset = get_dataset(opt)\n",
        "    sampler = get_bal_sampler(dataset) if opt.class_bal else None\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                              batch_size=opt.batch_size,\n",
        "                                              shuffle=shuffle,\n",
        "                                              sampler=sampler,\n",
        "                                              num_workers=int(opt.num_threads))\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "wKRtYFumiu_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}